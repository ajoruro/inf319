vboxuser@xubuntu:~$ su -
Password: 
root@xubuntu:~# cd "/home/vboxuser/Downloads/"
root@xubuntu:/home/vboxuser/Downloads# start-master.sh
org.apache.spark.deploy.master.Master running as process 5292.  Stop it first.
root@xubuntu:/home/vboxuser/Downloads# start-slave.sh spark://your-server-ip:7077
starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-xubuntu.out
root@xubuntu:/home/vboxuser/Downloads# spark-shell
22/12/11 17:20:30 WARN Utils: Your hostname, xubuntu resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
22/12/11 17:20:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.11-2.4.6.jar) to method java.nio.Bits.unaligned()
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
22/12/11 17:20:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/12/11 17:20:55 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Spark context Web UI available at http://10.0.2.15:4041
Spark context available as 'sc' (master = local[*], app id = local-1670797255471).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.6
      /_/
         
Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 11.0.17)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SparkSession

scala> val spark: SparkSession = SparkSession.builder().master("local[*]").appName("simple-app").getOrCreate()
22/12/11 17:22:40 WARN SparkSession$Builder: Using an existing SparkSession; some spark core configurations may not take effect.
spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@f500481

scala> import org.apache.spark.sql.Datase   
<console>:24: error: object Datase is not a member of package org.apache.spark.sql
       import org.apache.spark.sql.Datase
              ^

scala> import org.apache.spark.sql.Dataset 
import org.apache.spark.sql.Dataset

scala> val dataSet: Dataset[String] = spark.read.textFile("2019-Dec.csv")
dataSet: org.apache.spark.sql.Dataset[String] = [value: string]

scala> import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.DataFrame

scala> val df: DataFrame = dataSet.toDF()
df: org.apache.spark.sql.DataFrame = [value: string]

scala> import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.StreamingContext

scala> import org.apache.spark.SparkContext
import org.apache.spark.SparkContext

scala> import org.apache.spark.streaming.{Seconds,StreamingContext}
import org.apache.spark.streaming.{Seconds, StreamingContext}

scala> val streamingContext: StreamingContext = new StreamingContext(sparkContext,Seconds(20))
<console>:30: error: not found: value sparkContext
       val streamingContext: StreamingContext = new StreamingContext(sparkContext,Seconds(20))
                                                                     ^

scala> val streamingContext: StreamingContext = new StreamingContext(SparkContext,Seconds(20))
<console>:30: error: overloaded method constructor StreamingContext with alternatives:
  (path: String,sparkContext: org.apache.spark.SparkContext)org.apache.spark.streaming.StreamingContext <and>
  (path: String,hadoopConf: org.apache.hadoop.conf.Configuration)org.apache.spark.streaming.StreamingContext <and>
  (conf: org.apache.spark.SparkConf,batchDuration: org.apache.spark.streaming.Duration)org.apache.spark.streaming.StreamingContext <and>
  (sparkContext: org.apache.spark.SparkContext,batchDuration: org.apache.spark.streaming.Duration)org.apache.spark.streaming.StreamingContext
 cannot be applied to (org.apache.spark.SparkContext.type, org.apache.spark.streaming.Duration)
       val streamingContext: StreamingContext = new StreamingContext(SparkContext,Seconds(20))
                                                ^

scala> import org.apache.spark.streaming.dstream.ReceiverInputDStream
import org.apache.spark.streaming.dstream.ReceiverInputDStream

scala> val lines: ReceiverInputDStream[String] = StreamingContext.socketTextStream("localhost", 9999)
<console>:31: error: value socketTextStream is not a member of object org.apache.spark.streaming.StreamingContext
       val lines: ReceiverInputDStream[String] = StreamingContext.socketTextStream("localhost", 9999)
                                                                  ^

scala> import org.apache.spark.rdd.RDD
import org.apache.spark.rdd.RDD

scala> val cadenas = Array("Docentes", "inteligenciaArtificial", "quefinal")
cadenas: Array[String] = Array(Docentes, inteligenciaArtificial, quefinal)

scala> val cadenasRDD = sc.parallelize (cadenas)
cadenasRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:35

scala> cadenasRDD.collect()
java.lang.IllegalArgumentException: Unsupported class file major version 55
  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:166)
  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:148)
  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:136)
  at org.apache.xbean.asm6.ClassReader.<init>(ClassReader.java:237)
  at org.apache.spark.util.ClosureCleaner$.getClassReader(ClosureCleaner.scala:50)
  at org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:845)
  at org.apache.spark.util.FieldAccessFinder$$anon$4$$anonfun$visitMethodInsn$7.apply(ClosureCleaner.scala:828)
  at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)
  at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)
  at scala.collection.mutable.HashMap$$anon$1$$anonfun$foreach$2.apply(HashMap.scala:134)
  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)
  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)
  at scala.collection.mutable.HashMap$$anon$1.foreach(HashMap.scala:134)
  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)
  at org.apache.spark.util.FieldAccessFinder$$anon$4.visitMethodInsn(ClosureCleaner.scala:828)
  at org.apache.xbean.asm6.ClassReader.readCode(ClassReader.java:2175)
  at org.apache.xbean.asm6.ClassReader.readMethod(ClassReader.java:1238)
  at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:631)
  at org.apache.xbean.asm6.ClassReader.accept(ClassReader.java:355)
  at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:272)
  at org.apache.spark.util.ClosureCleaner$$anonfun$org$apache$spark$util$ClosureCleaner$$clean$14.apply(ClosureCleaner.scala:271)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:271)
  at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:163)
  at org.apache.spark.SparkContext.clean(SparkContext.scala:2326)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2100)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)
  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)
  ... 50 elided

scala> 

